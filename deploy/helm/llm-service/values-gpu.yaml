servingRuntime:
  name: vllm-serving-runtime
  knativeTimeout: 60m
  recommendedAccelerators:
  - nvidia.com/gpu
  image: quay.io/ecosystem-appeng/vllm:openai-v0.8.3
  env:
  - name: HOME
    value: /vllm
  - name: HF_TOKEN
    valueFrom:
      secretKeyRef:
        key: HF_TOKEN
        name: huggingface-secret
  volumeMounts:
  - name: shm
    mountPath: /dev/shm
  - name: vllm-home
    mountPath: /vllm
  volumes:
  - name: shm
    emptyDir:
      medium: Memory
      sizeLimit: 2Gi
  - name: vllm-home
    emptyDir:
      sizeLimit: 5Gi

models:
  llama-3-2-3b-instruct:
    id: meta-llama/Llama-3.2-3B-Instruct
    enabled: true
    storageSize: 50Gi
    args:
    - --enable-auto-tool-choice
    - --chat-template
    - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
    - --tool-call-parser
    - llama3_json
    - --max-model-len
    - "30544"

  llama-guard-3-8b:
    id: meta-llama/Llama-Guard-3-8B
    enabled: true
    storageSize: 50Gi
    args:
    - --max-model-len
    - "14336"
