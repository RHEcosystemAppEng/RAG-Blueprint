# Default values for rag-ui.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: quay.io/ecosystem-appeng/llamastack-dist-ui
  pullPolicy: IfNotPresent
  tag: 0.1.9

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: false

podAnnotations: {}
podLabels: {}

podSecurityContext: {}

securityContext: {}

service:
  type: ClusterIP
  port: 8501

env:
  - name: LLAMA_STACK_ENDPOINT
    value: 'http://llamastack:8321'
  - name: MCP_SERVERS_CONFIG_FILE
    value: "/app-config/mcp_servers_config.yaml"

ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []

resources: {}

livenessProbe:
  httpGet:
    path: /
    port: http
readinessProbe:
  httpGet:
    path: /
    port: http

autoscaling:
  enabled: false

# Additional volumes on the output Deployment definition.
volumes:
  - configMap:
      defaultMode: 420
      name: mcp-servers-config
    name: mcp-servers-config-volume

# Additional volumeMounts on the output Deployment definition.
volumeMounts:
  - mountPath: /app-config
    name: mcp-servers-config-volume
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

nodeSelector: {}

tolerations: []

affinity: {}

llama-stack:
  models:
    llama-3-2-1b-instruct:
      id:
        envVarName: LLAMA_INSTRUCT_MODEL_ID
        envVarValue: meta-llama/Llama-3.2-1B-Instruct
      url:
        envVarName: LLAMA_INSTRUCT_VLLM_URL
        envVarValue: auto
    llama-guard-3-1b:
      registerShield: true
      id:
        envVarName: LLAMA_GUARD_MODEL_ID
        envVarValue: meta-llama/Llama-Guard-3-1B
      url:
        envVarName: LLAMA_GUARD_VLLM_URL
        envVarValue: auto

## llama-serve and safety-model were replaced by llm-service, set replicaCount=0 ##
llama-serve:
  replicaCount: 0
  extraArgs:
    - --model
    - meta-llama/Llama-3.2-1B-Instruct
  tolerations: []
  resources: null
  image:
    repository: public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo
    pullPolicy: IfNotPresent
    tag: v0.8.1

safety-model:
  replicaCount: 0
  extraArgs:
    - --model
    - meta-llama/Llama-Guard-3-1B
  tolerations: []
  resources: null
  image:
    repository: public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo
    pullPolicy: IfNotPresent
    tag: v0.8.1
