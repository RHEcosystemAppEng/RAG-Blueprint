global:
  models:
    llama-3-2-3b-instruct:
      id: meta-llama/Llama-3.2-3B-Instruct
      enabled: false
      inferenceService:
        args:
        - --enable-auto-tool-choice
        - --chat-template
        - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
        - --tool-call-parser
        - llama3_json
        - --max-model-len
        - "30544"
      llamaStack:
        modelIdEnvName: LLAMA_INSTRUCT_MODEL
        modelUrl:
          envName: LLAMA_INSTRUCT_VLLM_URL
          envValue: auto
    llama-guard-3-8b:
      id: meta-llama/Llama-Guard-3-8B
      enabled: false
      inferenceService:
        args:
        - --max-model-len
        - "14336"
      llamaStack:
        modelIdEnvName: LLAMA_GUARD_MODEL
        modelUrl:
          envName: LLAMA_GUARD_VLLM_URL
          envValue: auto
        registerShield: true

llm-service:
  servingRuntime:
    image: quay.io/ecosystem-appeng/vllm:openai-v0.8.3
    recommendedAccelerators:
    - nvidia.com/gpu
