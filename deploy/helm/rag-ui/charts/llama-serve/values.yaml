# Default values for llama-serve.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: vllm/vllm-openai
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "latest"

imagePullSecrets: []
nameOverride: "vllm"
fullnameOverride: "vllm"

podAnnotations: {}
podLabels: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 8000

resources:
  limits:
    nvidia.com/gpu: "1"
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi
args:
  - --model
  - meta-llama/Llama-3.2-3B-Instruct
  - --enable-auto-tool-choice
  - --chat-template
  - /app/tool_chat_template_llama3.2_json.jinja
  - --tool-call-parser
  - llama3_json
  - --port
  - "8000"
  - '--max-model-len'
  - '8192'

serviceAccount:
  create: false

env:
  - name: VLLM_PORT
    value: "8000"
  - name: HUGGING_FACE_HUB_TOKEN
    valueFrom:
      secretKeyRef:
        key: HF_TOKEN
        name: huggingface-secret
# Additional volumes on the output Deployment definition.
volumes:
  - emptyDir: {}
    name: hf-cache
  - configMap:
      defaultMode: 420
      name: template
    name: chat-template
  - emptyDir: {}
    name: config

# Additional volumeMounts on the output Deployment definition.
volumeMounts:
  - mountPath: /.cache
    name: hf-cache
  - mountPath: /app
    name: chat-template
  - mountPath: /.config
    name: config


nodeSelector: {}

tolerations:
  - effect: NoSchedule
    key: g6e-gpu
    value: 'true'

affinity: {}
