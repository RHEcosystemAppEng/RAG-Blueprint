# PIPELINE DEFINITION
# Name: rag-docling-component
# Description: Process documents, convert them to chunks, and store in a vector database.
# Inputs:
#    document_path: system.Artifact
#    embedding_dimension: int [Default: 384.0]
#    embedding_model: str [Default: 'all-MiniLM-L6-v2']
#    llama_stack_url: str [Default: 'http://localhost:8321']
#    provider_id: str [Default: 'pgvector']
#    vector_db_id: str [Default: 'pgvector']
# Outputs:
#    Output: int
#    metrics_path: system.Artifact
components:
  comp-rag-docling-component:
    executorLabel: exec-rag-docling-component
    inputDefinitions:
      artifacts:
        document_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: Path to the document or a JSON file containing document paths
      parameters:
        embedding_dimension:
          defaultValue: 384.0
          description: Dimension size for embeddings
          isOptional: true
          parameterType: NUMBER_INTEGER
        embedding_model:
          defaultValue: all-MiniLM-L6-v2
          description: Model to use for embeddings
          isOptional: true
          parameterType: STRING
        llama_stack_url:
          defaultValue: http://localhost:8321
          description: URL for the Llama Stack API
          isOptional: true
          parameterType: STRING
        provider_id:
          defaultValue: pgvector
          description: Provider ID for vector database
          isOptional: true
          parameterType: STRING
        vector_db_id:
          defaultValue: pgvector
          description: ID for the vector database
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        metrics_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        Output:
          parameterType: NUMBER_INTEGER
deploymentSpec:
  executors:
    exec-rag-docling-component:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - rag_docling_component
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'llama-stack-client==0.1.9'\
          \ 'docling' 'docling-core' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef rag_docling_component(\n    document_path: InputPath(),\n   \
          \ metrics_path: OutputPath(),\n    llama_stack_url: str = \"http://localhost:8321\"\
          ,\n    embedding_model: str = \"all-MiniLM-L6-v2\",\n    embedding_dimension:\
          \ int = 384,\n    provider_id: str = \"pgvector\",\n    vector_db_id: str\
          \ = \"pgvector\"\n) -> int:\n    \"\"\"\n    Process documents, convert\
          \ them to chunks, and store in a vector database.\n\n    Args:\n       \
          \ document_path (InputPath): Path to the document or a JSON file containing\
          \ document paths\n        metrics_path (OutputPath): Path to write processing\
          \ metrics\n        llama_stack_url (str): URL for the Llama Stack API\n\
          \        embedding_model (str): Model to use for embeddings\n        embedding_dimension\
          \ (int): Dimension size for embeddings\n        provider_id (str): Provider\
          \ ID for vector database\n        vector_db_id (str): ID for the vector\
          \ database\n\n    Returns:\n        int: Number of documents processed\n\
          \    \"\"\"\n    # === Step 1: Configure Llama Stack client ===\n    client\
          \ = LlamaStackClient(base_url=llama_stack_url)  # Assumes LLAMA_STACK_API_KEY\
          \ is set in env\n\n    # === Step 2: Process document path ===\n    # The\
          \ document_path could be a single file or a JSON file with multiple document\
          \ paths\n    documents = []\n    if os.path.isfile(document_path):\n   \
          \     # Check if this is a JSON file with a list of document paths\n   \
          \     if document_path.endswith('.json'):\n            try:\n          \
          \      with open(document_path, 'r') as f:\n                    file_data\
          \ = json.load(f)\n                    if isinstance(file_data, list):\n\
          \                        documents = file_data\n                    elif\
          \ isinstance(file_data, dict) and 'document_paths' in file_data:\n     \
          \                   documents = file_data['document_paths']\n          \
          \          elif isinstance(file_data, dict) and 'documents' in file_data:\n\
          \                        documents = file_data['documents']\n          \
          \          else:\n                        documents = [document_path]\n\
          \            except json.JSONDecodeError:\n                documents = [document_path]\n\
          \        else:\n            documents = [document_path]\n\n    # === Step\
          \ 3: Convert, Chunk, and Prepare Documents ===\n    # converter format option\
          \ for the pictures on pdf to be generated as base64\n    pipeline_options\
          \ = PdfPipelineOptions()\n    pipeline_options.generate_picture_images =\
          \ True\n    converter = DocumentConverter(\n                format_options={\n\
          \                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n\
          \            }\n    )\n    chunker = HybridChunker()\n    llama_documents:\
          \ list[LlamaStackDocument] = []\n    i = 0\n    processing_metrics = {\n\
          \        \"document_count\": len(documents),\n        \"processed_documents\"\
          : [],\n        \"failed_documents\": []\n    }\n\n    for file_path in documents:\n\
          \        print(f\"Processing {file_path}...\")\n        try:\n         \
          \   docling_doc = converter.convert(source=file_path).document\n       \
          \     chunks = chunker.chunk(docling_doc)\n            chunk_count = 0\n\
          \n            for chunk in chunks:\n                if any(\n          \
          \          c.label in [DocItemLabel.TEXT, DocItemLabel.PARAGRAPH]\n    \
          \                for c in chunk.meta.doc_items\n                ):\n   \
          \                 i += 1\n                    chunk_count += 1\n       \
          \             llama_documents.append(\n                        LlamaStackDocument(\n\
          \                            document_id=f\"doc-{i}\",\n               \
          \             content=chunk.text,\n                            mime_type=\"\
          text/plain\",\n                            metadata={\"source\": file_path},\n\
          \                        )\n                    )\n\n            processing_metrics[\"\
          processed_documents\"].append({\n                \"file\": file_path,\n\
          \                \"chunks\": chunk_count\n            })\n        except\
          \ Exception as e:\n            error_message = str(e)\n            print(f\"\
          Error processing {file_path}: {error_message}\")\n            processing_metrics[\"\
          failed_documents\"].append({\n                \"file\": file_path,\n   \
          \             \"error\": error_message\n            })\n\n    total_chunks\
          \ = len(llama_documents)\n    processing_metrics[\"total_chunks\"] = total_chunks\n\
          \    print(f\"Total valid documents prepared: {total_chunks}\")\n\n    #\
          \ === Step 4: Create Vector DB ===\n    try:\n        client.vector_dbs.register(\n\
          \            vector_db_id=vector_db_id,\n            embedding_model=embedding_model,\n\
          \            embedding_dimension=embedding_dimension,\n            provider_id=provider_id,\n\
          \        )\n        print(f\"Vector DB registered successfully: {vector_db_id}\"\
          )\n        processing_metrics[\"vector_db_registration\"] = \"success\"\n\
          \n    except Exception as e:\n        error_message = str(e)\n        print(f\"\
          Failed to register vector DB '{vector_db_id}': {error_message}\")\n    \
          \    processing_metrics[\"vector_db_registration\"] = {\n            \"\
          status\": \"failed\",\n            \"error\": error_message\n        }\n\
          \n    # === Step 5: Insert into Vector DB ===\n    try:\n        client.tool_runtime.rag_tool.insert(\n\
          \            documents=llama_documents,\n            vector_db_id=vector_db_id,\n\
          \            chunk_size_in_tokens=512,\n        )\n        print(\"Documents\
          \ successfully inserted into the vector DB.\")\n        processing_metrics[\"\
          vector_db_insertion\"] = \"success\"\n\n    except Exception as e:\n   \
          \     error_message = str(e)\n        print(f\"Error inserting documents\
          \ into RAG tool: {error_message}\")\n        processing_metrics[\"vector_db_insertion\"\
          ] = {\n            \"status\": \"failed\",\n            \"error\": error_message\n\
          \        }\n\n    # Write metrics to output\n    with open(metrics_path,\
          \ 'w') as f:\n        json.dump(processing_metrics, f, indent=2)\n\n   \
          \ return total_chunks\n\n"
        image: python:3.9
pipelineInfo:
  name: rag-docling-component
root:
  dag:
    outputs:
      artifacts:
        metrics_path:
          artifactSelectors:
          - outputArtifactKey: metrics_path
            producerSubtask: rag-docling-component
      parameters:
        Output:
          valueFromParameter:
            outputParameterKey: Output
            producerSubtask: rag-docling-component
    tasks:
      rag-docling-component:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-rag-docling-component
        inputs:
          artifacts:
            document_path:
              componentInputArtifact: document_path
          parameters:
            embedding_dimension:
              componentInputParameter: embedding_dimension
            embedding_model:
              componentInputParameter: embedding_model
            llama_stack_url:
              componentInputParameter: llama_stack_url
            provider_id:
              componentInputParameter: provider_id
            vector_db_id:
              componentInputParameter: vector_db_id
        taskInfo:
          name: rag-docling-component
  inputDefinitions:
    artifacts:
      document_path:
        artifactType:
          schemaTitle: system.Artifact
          schemaVersion: 0.0.1
        description: Path to the document or a JSON file containing document paths
    parameters:
      embedding_dimension:
        defaultValue: 384.0
        description: Dimension size for embeddings
        isOptional: true
        parameterType: NUMBER_INTEGER
      embedding_model:
        defaultValue: all-MiniLM-L6-v2
        description: Model to use for embeddings
        isOptional: true
        parameterType: STRING
      llama_stack_url:
        defaultValue: http://localhost:8321
        description: URL for the Llama Stack API
        isOptional: true
        parameterType: STRING
      provider_id:
        defaultValue: pgvector
        description: Provider ID for vector database
        isOptional: true
        parameterType: STRING
      vector_db_id:
        defaultValue: pgvector
        description: ID for the vector database
        isOptional: true
        parameterType: STRING
  outputDefinitions:
    artifacts:
      metrics_path:
        artifactType:
          schemaTitle: system.Artifact
          schemaVersion: 0.0.1
    parameters:
      Output:
        parameterType: NUMBER_INTEGER
schemaVersion: 2.1.0
sdkVersion: kfp-2.12.1
